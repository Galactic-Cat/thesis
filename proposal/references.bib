@article{haskell,
  title={Haskell 2010 language report},
  author={Marlow, Simon and others},
  journal={\url{https://www.haskell.org/definition/haskell2010.pdf}},
  year={2010}
}

@article{margossian2019review,
  title={A review of automatic differentiation and its efficient implementation},
  author={Margossian, Charles C},
  journal={Wiley interdisciplinary reviews: data mining and knowledge discovery},
  volume={9},
  number={4},
  pages={e1305},
  year={2019},
  publisher={Wiley Online Library}
}

@article{elliot2018essence,
    author = {Elliott, Conal},
    title = {The Simple Essence of Automatic Differentiation},
    year = {2018},
    issue_date = {September 2018},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {2},
    number = {ICFP},
    url = {https://doi.org/10.1145/3236765},
    doi = {10.1145/3236765},
    abstract = {Automatic differentiation (AD) in reverse mode (RAD) is a central component of deep learning and other uses of large-scale optimization. Commonly used RAD algorithms such as backpropagation, however, are complex and stateful, hindering deep understanding, improvement, and parallel execution. This paper develops a simple, generalized AD algorithm calculated from a simple, natural specification. The general algorithm is then specialized by varying the representation of derivatives. In particular, applying well-known constructions to a naive representation yields two RAD algorithms that are far simpler than previously known. In contrast to commonly used RAD implementations, the algorithms defined here involve no graphs, tapes, variables, partial derivatives, or mutation. They are inherently parallel-friendly, correct by construction, and usable directly from an existing programming language with no need for new data types or programming style, thanks to use of an AD-agnostic compiler plugin.},
    journal = {Proc. ACM Program. Lang.},
    month = {jul},
    articleno = {70},
    numpages = {29},
    keywords = {category theory, program calculation, automatic differentiation}
}

@article{baydin2018survey,
  author  = {Atilim Gunes Baydin and Barak A. Pearlmutter and Alexey Andreyevich Radul and Jeffrey Mark Siskind},
  title   = {Automatic Differentiation in Machine Learning: a Survey},
  journal = {Journal of Machine Learning Research},
  year    = {2018},
  volume  = {18},
  number  = {153},
  pages   = {1--43},
  url     = {http://jmlr.org/papers/v18/17-468.html}
}

@article{wang2019demystifying,
  title={Demystifying differentiable programming: Shift/reset the penultimate backpropagator},
  author={Wang, Fei and Zheng, Daniel and Decker, James and Wu, Xilun and Essertel, Gr{\'e}gory M and Rompf, Tiark},
  journal={Proceedings of the ACM on Programming Languages},
  volume={3},
  number={ICFP},
  pages={1--31},
  year={2019},
  publisher={ACM New York, NY, USA}
}

@article{krawiec2022provably,
  title={Provably correct, asymptotically efficient, higher-order reverse-mode automatic differentiation.},
  author={Krawiec, Faustyna and Jones, Simon Peyton and Krishnaswami, Neel and Ellis, Tom and Eisenberg, Richard A and Fitzgibbon, Andrew W},
  journal={Proc. ACM Program. Lang.},
  volume={6},
  number={POPL},
  pages={1--30},
  year={2022}
}

@article{smeding2022efficient,
  title={Efficient Dual-Numbers Reverse AD via Well-Known Program Transformations},
  author={Smeding, Tom and V{\'a}k{\'a}r, Matthijs},
  journal={arXiv preprint arXiv:2207.03418},
  year={2022}
}

@book{griewank2008evaluating,
  title={Evaluating derivatives: principles and techniques of algorithmic differentiation},
  author={Griewank, Andreas and Walther, Andrea},
  year={2008},
  publisher={SIAM}
}

@article{paszke2021getting,
  title={Getting to the point. index sets and parallelism-preserving autodiff for pointful array programming},
  author={Paszke, Adam and Johnson, Daniel and Duvenaud, David and Vytiniotis, Dimitrios and Radul, Alexey and Johnson, Matthew and Ragan-Kelley, Jonathan and Maclaurin, Dougal},
  journal={arXiv preprint arXiv:2104.05372},
  year={2021}
}

@article{schenck2022ad,
  title={AD for an Array Language with Nested Parallelism},
  author={Schenck, Robert and R{\o}nning, Ola and Henriksen, Troels and Oancea, Cosmin E},
  journal={arXiv preprint arXiv:2202.10297},
  year={2022}
}

@article{shaikhha2019efficient,
  author = {Shaikhha, Amir and Fitzgibbon, Andrew and Vytiniotis, Dimitrios and Peyton Jones, Simon},
  title = {Efficient Differentiable Programming in a Functional Array-Processing Language},
  year = {2019},
  issue_date = {August 2019},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {3},
  number = {ICFP},
  url = {https://doi.org/10.1145/3341701},
  doi = {10.1145/3341701},
  abstract = {We present a system for the automatic differentiation (AD) of a higher-order functional array-processing language. The core functional language underlying this system simultaneously supports both source-to-source forward-mode AD and global optimisations such as loop transformations. In combination, gradient computation with forward-mode AD can be as efficient as reverse mode, and that the Jacobian matrices required for numerical algorithms such as Gauss-Newton and Levenberg-Marquardt can be efficiently computed.},
  journal = {Proc. ACM Program. Lang.},
  month = {jul},
  articleno = {97},
  numpages = {30},
  keywords = {Loop Fusion, Linear Algebra, Optimising Compilers, Differentiable Programming, Code Motion}
}