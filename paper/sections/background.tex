\section{Background}
    \subsection{Automatic Differentiation}
        Automatic Differentiation (AD), like the name suggests, involves programmatically finding the derivative of some programmed function \cite{margossian2019review}.
        The other main method for programmatically finding the derivative of a function is numerical differentiation, which uses the finite difference method.
        By adjusting the input(s) to the function by a very small number, we can see the effect on the output(s) of the function.
        Unfortunately, due to the way real numbers are represented using floating-point computation, this method is prone to round-off error (or truncation error).
        AD avoids this by actually performing the differentiation on a program, to produce the differentiated program.
        This is very similar to how a human would differentiate a mathematical function (sometimes called symbolic or manual differentiation), but performed on a computer program.
        AD makes very explicit use of the chain rule of partial derivatives of compound functions, which states that we can combine partial derivatives of parts of the function together into the complete derivative.
        For instance, when if some variable $z$ depends on the variable $y$, which then depends on the variable $x$, we can obtain the derivative of $x$ under $z$ using the chain rule as in Equation \ref{eq:chainrule}.
        \begin{equation} \label{eq:chainrule}
            \frac{dz}{dx}=\frac{\partial z}{\partial y}\cdot\frac{\partial y}{\partial x}
        \end{equation}

        To actually implement automatic differentiation, we seek to break the target program down to its most basic mathematical operations.
        Then we can find the partial derivatives of these operations, and use the chain rule to combine them together into the derivative of the program.
        There are two main ways to actually resolves these derivatives: using either forward accumulation or backward/reverse accumulation.
        When applied in AD implementations these are commonly respectively referred to as forward-mode and reverse-mode.
        Both methods are described in the 1986 paper ``The arithmetic of differentiation'' by B. Rall \cite{rall1986arithmetic}.
        
        In forward-mode AD we move through the program to differentiate in normal execution order.
        By knowing which input variable we wish to differentiate, we can compute every step of the derivative as our inputs are used by the program.
        Rall demonstrates this using a method known as dual-numbers, where each real number is represented by a pair of numbers, similar to complex numbers.
        In dual-numbers, the first number in the pair represents the primal part of the number, whereas the second number represents the derivative part (called the tangent in forward-mode).
        When we modify these numbers through arithmetic operations, we can operate on the primal parts as normal, and use derivative rules to calculate the derivative of the result using the tangent parts.
        An example of this is given in Equation \ref{eq:dualnumbers}, where $\dot{a}$ is the tangent part of some real number $a$.
        \begin{equation} \label{eq:dualnumbers}
            (a,\dot{a})\cdot(b,\dot{b})=(a\cdot b, \dot{a}\cdot b+\dot{b}\cdot a)
        \end{equation}
        Now we can find the derivative of some input $x_i$ by setting $\dot{x_i}$ to $1$, setting the tangents of all other inputs to $0$, and just running through the program calculating tangents as we go.
        The tangent part of the output value(s) is also the calculated derivative of the whole program.

        While forward-mode AD is fairly straightforward, it comes with some drawbacks.
        The main one being that for a function $f:\mathbb{R}^n\to\mathbb{R}^m$ with $n$ inputs and $m$ outputs, to get the effect of each input variable on each output variable, we would need to perform $n$ passes over the function, one for each input variable (or we need to track $n$ tangent parts for each step).
        This is can be cumbersome, especially if $n$ is much larger than $m$.
        For those cases, we might be better off with reverse accumulation, or reverse-mode.

        In reverse-mode, we peg the derivative part of one of our outputs with some seed (often $1$), and set the derivative parts of the other outputs to $0$.
        These derivative parts are generally referred to as adjoints instead of tangents in reverse-mode.
        When the outputs are set, we can work our way back through the function, calculating the derivative parts from the output to the input.
        Intuitively, this computes the change of the inputs for a small change in an output.
        Practically, the idea of working back through a program requires some way of knowing where the outputs came from (a sort of dependency structure).
        This then requires a forward pass, to find this structure, and often to setup any dual-numbers or other implementation details.
        And while reverse-mode is definitely harder to implement, it also provides us with a way to calculate the sensitivity of all inputs to an output, which is much more efficient for functions with many more inputs than outputs (which can be quite common in certain fields like neural networks).

        In mathematical terms, calculating the partial derivative of one input with regards to one output, means calculating one cell in the Jacobian, the matrix of all partial derivatives.
        For a function $f:\mathbb{R}^n\to\mathbb{R}^m$ with $n$ inputs and $m$ outputs, the Jacobian would be a $n\times m$ matrix.
        Here a column $i$ represents the derivatives of $\frac{\partial\vec{f}}{\partial x_i}$, where $\vec{f}$ are all outputs of $f$, and $x_i$ represents a single input.
        A row $j$ then represents the derivatives of $\nabla f_j=\frac{\partial f_j}{\partial\vec{x}}$, where $\vec{x}$ are all inputs of $f$, and $\nabla f_j$ is also known as the gradient of $f_j$.
        This is also shown in Equation \ref{eq:jacobian}, showing the Jacobian for some function $f$ with $n$ inputs ($x_1,\dots,x_n$) and 
        An important take-away here is that forward-mode computes the derivatives of all outputs with regards to a single input, so a column in the Jacobian, and reverse-mode computes the derivatives of all inputs with regards to a single output, so a row in the Jacobian.
        Again, forward-mode is more efficient when we have more outputs than inputs or when the Jacobian has more columns than rows, and the reverse-mode is more efficient for functions with more inputs than outputs or for Jacobians with more rows than columns.

        \begin{equation} \label{eq:jacobian}
            J_f=\left[\frac{\partial \vec{f}}{\partial x_1},\dots,\frac{\partial \vec{f}}{\partial x_n}\right]=\begin{bmatrix}\nabla f_1\\\vdots\\\nabla f_m\end{bmatrix}=\begin{bmatrix}
                \frac{\partial f_1}{\partial x_1} & \dots & \frac{\partial f_1}{\partial x_n}\\
                \vdots & \ddots & \vdots\\
                \frac{\partial f_m}{\partial x_1} & \dots & \frac{\partial f_m}{\partial x_n}
            \end{bmatrix}
        \end{equation}

        While it was previously shown that reverse-mode automatic differentiation could be executed in time equal to some constant multiple of the execution time of the primal program \cite{linnainmaa1976taylor}, it seemed that a constant multiple of the execution memory was also needed, which could become very expensive for large programs.
        However, in 1992, Andreas Griewank showed that by using a method called taping we could reduce the time and space complexity to a constant multiple of the log of the execution time \cite{griewank1992achieving}.
        In general the practice of taping refers to a form of tracing on the program we wish to differentiate, where we execute the program as normal and record all the steps and intermediate values in a first-in-last-out data structure referred to as a "tape" or Wengert list.
        In a second phase to the reverse-mode algorithm, the tape is then used to calculate the derivatives in question, which due to the first-in-last-out nature of the tape, is in the precise reverse of the execution order of the program.
        It should be noted that an input agnostic form of reverse-mode AD is also possible, where the primal program is transformed into a program that computes its derivative for any input, for which taping would only refer to storing the operations without any intermediate values.
        This is referred to as source transformation \cite{bischof2000computing}.
        An important advantage of taping is that by giving each variable and intermediate calculation a unique id we can avoid redundant execution, because we can just refer to the intermediate value or tangent/adjoint stored in the tape.
        
        For forward-mode AD, the evaluation of the derivative is done during execution. Like in 1996's FADBAD package, which provided both forward-mode and reverse-mode AD for C++ \cite{bendtsen1996fadbad}.
        The reverse-mode uses the taping method described by Griewank, implemented through a method called operator overloading.
        In forward-mode, operator overloading refers to providing the basic mathematical operators with methods that work on the numbers represented by a pair (of a primal part and a tangent part).
        For reverse-mode, operator overloading is used to rewrite the basic mathematical operators, so they record their use and intermediate values to a single tape data structure.
        A similar implementation was also provided by Griewank et al. in the 1996 package ADOL-C \cite{griewank1996algorithm}, again in 2001 using more efficient expression templates by Aubert et al. \cite{aubert2001automatic}, and later in 2014 by Robin Hogan \cite{hogan2014fast}.

        Source-code transformation is eventually also implemented, in the Tapenade AD program \cite{hascoet2013tapenade}.
        Tapenade adds derivative calculations to the code, but also employs lazy/delayed evaluation in the forward pass.
        This allows Tapenade to do some activity analysis, which in turn allows it to combine or discard some partial derivatives to be more efficient.
        It also implements a method called checkpointing, where part of the tape is stored to be restored and differentiated later.
        This, in theory, allows for differentiating programs of arbitrary size \cite{griewank2008evaluating}.

        More recently implementations, like Fei Wang et al. 2019 paper, have shown how to simplify reverse automatic differentiation using continuation passing style and delimited continuation \cite{wang2019demystifying}.
        This method uses dual numbers and cleverly overloads operators so they call the forward pass as a continuation and then perform the backwards pass on the returned value.
        This can be efficiently implemented in functional languages, which is important because taping required either mutability or additional time complexity.

        In 2022, Krawiec et al. show how reverse-mode AD can be extended efficiently to higher-order functional programs \cite{krawiec2022provably}.
        While the Wang paper also did this, Krawiec uses the functional nature to provide a correctness proof of the reverse-mode AD, something that had previously only been done on implementations that were either asymptotically inefficient or only worked on first-order languages.
        They do however need taping again to make it provable and efficient.\\
        V치k치r and Smeding provide a provably correct form of efficient higher-order reverse AD without taping in their 2022 paper \cite{vakar2022chad}, based on earlier work by Elliott in 2018 \cite{elliott2018simple}.

        And in 2022 as well, Schenck et al. show how to do both forward-mode and reverse-mode automatic differentiation on second-order array language with nested data parallelism \cite{schenck2022ad}.
        They do this by eliminating taping again, which forces sequential execution, by allowing potential redundant execution.
        But by limiting their AD implementation to second-order functional languages, they can largely avoid this redundancy with efficient program transformations on parallel operators.

        Finally, in 2023, Smeding en V치k치r bring back explicit dual-numbers to reverse AD \cite{smeding2023efficient}.
        However, instead of pairing each number with its computed adjoint, they instead pair it with a linear backpropagator function, which they can then later chain to get the full derivative.
        While this initially seems to eliminate the need for taping, they find that through optimizations they return to a concept that is very close to taping and show that it is in fact equivalent.

    \subsection{Functional Parallel Array Programming}
        A good starting point for functional parallel array programming was in 1992, with G. Belloch's paper on the parallel array programming language NESL \cite{blelloch1992nesl}.
        The language was strongly-typed and had no support for side-effects, making it basically a functional language.
        The main way to add parallelism was through the inherently data-parallel ``vectors'' the language introduces in lieu of lists.
        These vectors could also be nested, and functions could run in nested parallel on these vectors.
        Another major inclusion was to allow user-defined functions to be run (in parallel) on these vectors, making it possible to write more complex nested data-parallel algorithms than before.

        The functional language Haskell, saw the introduction of task-parallelism well before its first official release, through libraries like pH \cite{maessen1995semantics}.
        Some data-parallelism followed \cite{hill1995data, herrmann1999parallelization, ellmenreich2000application}, but this was limited to applying a function over a flat array.
        However, in 2001 nested data-parallelism was introduced to Haskell by the NEPAL project by Chakravarty et al. \cite{chakravarty2001nepal}.
        The paper largely focusses on reimplementing NESL as a Haskell library, but creates a much more expressive data-parallel language doing so.
        This is because NESL was rather limited in scope, whereas Haskell was already a fully-fledged functional programming language.
        Two important concepts come to the forefront in the NEPAL paper, namely flattening and fusion.
        Both in NESL and in NEPAL, higher-dimensional nested parallelism is ``flattened'' to a single distributed parallel operation.
        In NESL, this meant that data-types had to be limited to tuples and the vectors it introduced, to make sure this flattening operation worked correctly.
        Since then however, Keller and Chakravarty had showed this flattening transformation could also be applied more generally \cite{keller1999transformation,keller1998flattening,chakravarty2000more}.
        This allowed them to apply the nested data parallelism of NESL to Haskell.
        Furthermore, they also showed that in combination with fusion it could produce efficient code for distributed machines \cite{chakravarty2001functional}.
        Fusion is where multiple separate parallel operations are combined into a single parallel operation, which can greatly improve performance of complicated parallel programs.
        This is important because many operations on arrays introduce the need for intermediate arrays to be computed.
        Doing this in parallel leads to more problems, as these implementations rely on gang parallelism, where the parallel threads remain in lockstep with each other \cite{feitelson1996packing}.
        Fusion helps us here, as we can reduce the number of intermediate arrays to be generated, as we can calculate the results of multiple operations at once \cite{keller1999distributed,chakravarty2007data}.
        
        All this work culminated in 2007's Data Parallel Haskell (DPH) \cite{peyton2008harnessing}, by Peyton et al.
        Its main feature was the parallel array, that like NESL's vectors, were the main way of adding parallelism to a program.
        However, these parallel arrays could now hold any type, such as other arrays or functions, like Haskell's native (non-parallel) lists.
        Furthermore, DPH provides parallel variants of Haskell's native list functions, and a parallel alternative to Haskell's list comprehensions.
        The main difference between Haskell's native lists and DPH's parallel arrays (besides the parallelism) was that evaluating any value in a parallel array would require evaluation on all the array's elements, whereas Haskell as a lazy language would not normally do that.
        This is to be expected, as parallelism becomes meaningless if it is only applied to a single entry of an array.

        Outside of Haskell, a functional array-programming dialect of C was developed: Single Assignment C (SAC) \cite{scholz1994single, scholz2003single, grelck2005generic}.
        It would go on to distinguish itself as a functional array programming language in a style more familiar to programmers of imperative languages (like C).
        The main mechanic in SAC is the with-loop, which takes a generator that dictates a looping mechanism and an operation that dictates the return value.
        These operations can be functions like ``fold'' to reduce the rank of an array, or ``genarray'' to generate new (multidimensional) arrays.
        Besides the imperative style, the main draw of SAC is its performance comparable to Fortran and C, while its programs are generally more concise (for intensive numerical applications.)

        In 2010, Keller, Chakravarty, et al. presented a new data-parallel approach for Haskell in ``Regular, Shape-polymorphic, Parallel Arrays in Haskell'' \cite{keller2010regular}.
        Previous approaches had focussed on irregular arrays, where on array could contain arrays of different lengths.
        The library Repa, introduced in this paper, was made for regular arrays where arrays of each nested rank are the same size.
        However, this allows the library to be purely functional, and allows support for shape polymorphism.
        In shape polymorphism, the type of a collection is fixed (unlike in type polymorphism), but the shape of the collection is not \cite{jay1994shapely}.
        For instance, under shape polymorphism a function may be applied to either a flat array, or a 10-dimensional one.
        While shape polymorphism for functional arrays had been implemented before in SAC, Repa implemented it by embedding it into Haskell's type system, whereas the SAC implementation had required a purpose-built compiler.
        This also allowed programmers to more easily see and control the shapes of their multidimensional parallel arrays, and build their own shape polymorphic parallel functions.

        In 2011, Repa was succeeded by the Accelerate project \cite{chakravarty2011accelerating}.
        Accelerate is a library for Haskell, aimed specifically on bringing parallel array programming to modern GPUs.
        It mimicked many of Haskell's native list functions as parallel alternatives (run on the GPU), and used the typed shaped polymorphism from Repa.
        It also separated ``collective'' (array) computations and scalar computation by wrapping these in Haskell monads.
        Here, collective computations could include scalar computations, but not the other way around.
        This meant excluding nested and irregular data parallelism, which in turn allows Accelerate to efficiently run on GPUs (which are much more constrained than CPUs).
        
        Another interesting example of a parallel array programming language is Remora by Slepak et al. \cite{slepak2014array}
        The language, inspired by earlier array programming languages APL \cite{iverson1962programming} and J implements rank-polymorphism.
        Rank polymorphism is similar to shape polymorphism, but it annotates functions and operators with a specific array rank they can operate on, and was also present in Repa and Accelerate.
        Remember that scalars are considered rank 0 arrays, a flat array is rank 1, a matrix is rank 2, et cetera.
        In rank polymorphism, arguments are transformed (re-ranked) such that they are the rank required for a specific function or operator.
        Specifically, an operator defined for a certain rank, is automatically defined for any higher rank.
        With Remora, Slepak et al. tried to shed some light on the more ``murkier corners'' of the array-computational model.
        They do this by generalizing the array-computational model, which then allows them to both address some of the shortcomings of APL, but also allows them to extend the model to allow arrays of functions and arrays of arguments, which in turn allows for the parallel MIMD (multiple instruction, multiple data) architecture, rather than only SIMD (single instruction, multiple data) parallelism.

        Finally, a more recent parallel array programming language is Dex \cite{maclaurin2019dex,paszke2021getting}.
        Rather than avoiding loops and explicit indexing, like NESL, NEPAL, DPH, and Repa had all done, Dex suggests that these features might introduce more clarity, if only they were implemented correctly.
        The main idea is to treat index sets as types and arrays as functions.
        In reality this ``index comprehension'' can also be seen as functions that return arrays, and allow declaring iteration over multiple dimensions in a single line.
        The main advantage of this is that loops make some parallelism opportunities very explicit.
        Also when these index comprehensions are presented back-to-back, opportunities for fusion become fairly clear as well.
        In their paper, they also show that on some benchmark problems, Dex performs similarly to Futhark \cite{henriksen2017futhark}, a functional array programming language that was specifically designed to write performant parallel GPU code.
